{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df29115-a89e-48d5-97f7-3065f9bbfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6ce3f-2178-4114-acc0-cc20b25554e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lecture de tous les mots\n",
    "words = open('dataset/names.txt', 'r').read().splitlines()\n",
    "words [:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a36898-1c35-4e83-a45d-3a9df2878544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Création des dictionnaires\n",
    "chars = sorted(list(set(''.join(words)))) # estraction de chaque caracère de la liste de mots\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # Création d'un liste en commençant à l'indice 1 ou chaque caractère est associcé à un index\n",
    "stoi['.'] = 0 #jout du caractère \".\" à l'indice 0\n",
    "itos = {i:s for s,i in stoi.items()} # Création d'une liste inverse pour avoir le nombre et la corespondance du caractere\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c390758-3d4d-40f4-a874-530d13b928f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "#Construction du dataset\n",
    "block_size = 3 # taille du contexte dans cet exemple \n",
    "\n",
    "X, Y = [], [] #Création de deux liste X pour les entrées et Y pour la sortie attendue\n",
    "\n",
    "for w in words [:5]:\n",
    "    print (w)\n",
    "    context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "    for ch in w + '.': # Pour chauqe catactère du mot\n",
    "\n",
    "        ix = stoi[ch] # On récupère la valeur du nombre correspondant à la lettre\n",
    "        X.append(context) # On ajoute le contexte à la liste X\n",
    "        Y.append(ix) # On ajoute la valeur numérique du caractère à la liste Y\n",
    "        \n",
    "        #On affiche le contexte + ----> + le caractère du mot que l'on traite \n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        # La même chose mais on affiche les indexes\n",
    "        #print(''.join(str(i) for i in context), '--->', ix)\n",
    " \n",
    "        \n",
    "        \n",
    "        # On jette le premier élément du contexte et on d&calle tout a gauche\n",
    "        #et on ajoute la valeur numérique de la lettre dans le contexte\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec19080-9bac-4cc5-b7fe-cb6a28393e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a73b41e-d517-43af-878f-dd0c7e0ff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de la matrice des embeddings, ou chaque caractère est \n",
    "# représenté par une matrice a deux dimensions aléaroires pour commencer\n",
    "C = torch.randn((27, 2))\n",
    "#print (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16eae17-a2a4-4719-b13a-a5cdacf0301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "####### CONSTRUCTION DE LA COUCHE D'EMBEDDING ########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ba1ae9-523d-4ff3-b9d9-a739a36c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding de la matric X\n",
    "# A ce stade X contient les contexte de 3 index\n",
    "# Le réseau ne peut pas travailler avec ces indexes c'est pourquoi on va \n",
    "# embedder X pour donner un représentation mathématique des caractères via\n",
    "# des vecteurs denses\n",
    "# Avant embedding = X[i] = [5, 13, 13]\n",
    "# Après embedding = X[i] = [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# [\n",
    "      #C[5],    # embedding de 'e' → [1.7, -0.3]\n",
    "      #C[13],   # embedding de 'm' → [0.2, +1.5]\n",
    "      #C[13]    # embedding de 'm' → [0.2, +1.5]\n",
    "# ]\n",
    "emb = C[X]\n",
    "# On obtient un tenseur de 32 lignes (le nombre d'exemple dans notre training set), dans chaque ligne 3 éléments (contexte) \n",
    "# et pour chaque élément la version embedder donc une matrice de 2 dimensions\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb97c7e3-9f10-41e2-94a3-8c6fc98eff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######### CONSTRUCTION DE LA HIDDEN LAYER ############\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a78f30e-d7e3-43a0-900e-3160e368ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape emb : torch.Size([32, 3, 2])\n",
      "shape W1 : torch.Size([6, 100])\n",
      "shape b1 : torch.Size([100])\n",
      "shape emb.view(-1, 6) : torch.Size([32, 6])\n",
      "shape de h : torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "#Initialisatin des poids aléatoirement\n",
    "W1 = torch.randn((6, 100)) \n",
    "# 6 lignes car \n",
    "# 6 entrées car notre embedding contient 6 valeurs par ligne\n",
    "# 3 lettres de contexte * 2 embedding par lettre  \n",
    "# [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# La matrice est applatit avant d'être envoyé au neuronne pour ne faire q'un seul vecteur\n",
    "# [1.7, -0.3, 0.2, 1.5, 0.2, 1.5]   # vecteur de taille 6\n",
    "\n",
    "# 100 neuronnes cachés avec 6 poids => Taille initié à l'expérience :) \n",
    "\n",
    "#Initialisation des biais aléatoirement\n",
    "b1 = torch.randn(100)\n",
    "# 100 biais car 100 neuronnes\n",
    "#Chaque neuronne reçoit la valeur de l'embedding applati,\n",
    "\n",
    "\n",
    "# Il fait son calcul tanh(aX + b) et en sortie on a une valeur\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "\n",
    "#Explication du emb.view\n",
    "# A ce stade on veut muliplier une matrice emb de dimensions [32, 3, 2]\n",
    "# par la matrice W1 de dimension [6, 100]\n",
    "# Ce qui est imossible, la multiplication d'une matrice A par une matrice B impose\n",
    "# que le nombre de colonne de la matrice A soit égal au nombre de ligne de la matrice B\n",
    "# Il faut donc que la matrice emb ai 6 colonne (actuellement 3)\n",
    "# la fonction view permet, sans créer de nouveau tenseur en mémoire, de transoformer \n",
    "# la forme de notre matrice et pour garder une forme dynamique on indique -1 au niveau\n",
    "# de la ligne, torch sait alors que ce sera forcément 32 dans notre cas car il fait\n",
    "# autmatiquement le calcul en fonction de la taille du tenseur d'origine\n",
    "# origine 32 * 3 * 2 = 192 donc si on indique 6 il fait 192 / 6 pour déduire 32\n",
    "\n",
    "print('shape emb :', emb.shape)\n",
    "print('shape W1 :', W1.shape)\n",
    "print('shape b1 :', b1.shape)\n",
    "print('shape emb.view(-1, 6) :', emb.view(-1, 6).shape)\n",
    "\n",
    "# La shape de h sera égale au nombre de ligne de la matrice emb.view(-1, 6) >> 32\n",
    "# Et du nombre de colonne de la matrice W1 >> 100\n",
    "# Règle de la multiplication des matrices\n",
    "\n",
    "##  Règle mathématique d'addition de matrice, pour addition deux matrices il faut qu'elle soient de mêm taille :\n",
    "# L'addition de emb.view(-1, 6) @ W1  b1 à 1 dimension  est possible grace au\n",
    "# Broadcasting, qui va étendre automatiquement le plus petit vecteur pour que la forme soit compatible\n",
    "# b1 est un vecteru composé d'une ligne et de 100 valeurs (colonnes)\n",
    "# Pour permettre l'addition le broadcasting va répété virtuellement b1 sur 32 lignes\n",
    "print('shape de h :', h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8613d0-02a1-4f23-9ba2-0caa815583a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######## CONSTRUCTION DE LA COUCHE DE SORTIE #########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f21366d7-6e33-4238-ba11-b4b003eb6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisatin des poids aléatoirement\n",
    "W2 = torch.randn((100, 27)) \n",
    "# 100 lignes car \n",
    "# 100 sorties de notre hidden layer (shape de h : torch.Size([32, 100]))\n",
    "# 27 colonnes car 27 sortie étant données le nombre de caractère possible   \n",
    "\n",
    "#Initialisation des biais aléatoirement\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db8231-d0b2-4a22-bf49-0d21e96dfed9",
   "metadata": {},
   "source": [
    "######################################################\n",
    "#################### Forward Pass ####################\n",
    "######################################################\n",
    "\n",
    "# 27 Scores bruts non normalisés, un niveau \"d'energie attribué a chaque caractère\"\n",
    "logits = h @ W2 + b2\n",
    "logits.shape\n",
    "\n",
    "# On va calculer la probabilité\n",
    "    #counts = logits.exp() # Transforme chaque score brut en nombre strictement positif \n",
    "#on le divise par la valeur des sommes de toutes les colonnes pour en faire une moyenne »\n",
    "    #prob = counts / counts.sum(1, keepdims=True) ## Normalisation pour que la somme d'une ligne soit égale à 1\n",
    "# prob contient 32 lignes (nombre d'exemple) et 27 colonne représentant chauqe caractère et la probabilité affecté a chaque caractère\n",
    "# Création d'un vecteur de 32 valeurs dans le quel on stock la valeur prédite par le modèle\n",
    "# Pour chaque caractère qui était attendu (donc ce n'est forcément celui qui a eu la meilleur probabilité\n",
    "# Ce qui est normal, à ce stade le modèle n'est pas encore entrainé)\n",
    "    #preparationCalculLoss = prob[torch.arange(32), Y]\n",
    "    #print (preparationCalculLoss)\n",
    "\n",
    "# On prend la probabilité du caractère correct, on prend son log, on met un signe moins, on moyenne :\n",
    "# c’est la Negative Log-Likelihood, la mesure standard de “à quel point le modèle a eu tort”.\n",
    "# C'est cette valeur que l'on va tenter de minimiser pour améliorer notre modèle\n",
    "    #loss = -preparationCalculLoss.log().mean()\n",
    "    #print (loss.item())\n",
    "\n",
    "# La fonction F.cross_entropy(logits, y) faile calcul du neagative loglikelihood de façon plus efficiente\n",
    "# cross_entropy fait tout en une seule opération efficace au lieu de 4 quand on le fait manuellement\n",
    "# cross_entropy est NUMÉRIQUEMENT STABLE >> \n",
    "    # Si un logit = 50 → exp(50) ≈ 5 × 10²¹ → explosion\n",
    "    # Si un logit = –50 → exp(–50) ≈ 1.9e–22 → underflow\n",
    "        # ➡️ aucun overflow\n",
    "        # ➡️ aucun underflow\n",
    "        # ➡️ même si les logits sont énormes\n",
    "        # ➡️ gradients fiables\n",
    "# C’est plus rapide (implémenté en C++/CUDA)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print('Caclul de la loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b68bab2c-6ca6-4153-8b59-a6200f81c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on ne fait pas ça on a cette erreur\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "329784c4-4023-4d79-b108-ff3d0e7dd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.841652870178223\n",
      "12.190972328186035\n",
      "10.364726066589355\n",
      "9.19583511352539\n",
      "7.876575946807861\n",
      "7.002636909484863\n",
      "6.338400363922119\n",
      "5.7566046714782715\n",
      "5.264477252960205\n",
      "4.804817199707031\n",
      "4.403929710388184\n",
      "4.027704238891602\n",
      "3.7040207386016846\n",
      "3.4027419090270996\n",
      "3.1420130729675293\n",
      "2.8859381675720215\n",
      "2.662510871887207\n",
      "2.434684991836548\n",
      "2.2368736267089844\n",
      "2.032646656036377\n",
      "1.8584678173065186\n",
      "1.6803371906280518\n",
      "1.5371460914611816\n",
      "1.3858317136764526\n",
      "1.2780719995498657\n",
      "1.1552858352661133\n",
      "1.0732132196426392\n",
      "0.9778939485549927\n",
      "0.9087672233581543\n",
      "0.8344378471374512\n",
      "0.7799637913703918\n",
      "0.7249069213867188\n",
      "0.6888688802719116\n",
      "0.6497572660446167\n",
      "0.6278157234191895\n",
      "0.5975610017776489\n",
      "0.5833364725112915\n",
      "0.5582679510116577\n",
      "0.5479008555412292\n",
      "0.5274479389190674\n",
      "0.519249677658081\n",
      "0.5029134154319763\n",
      "0.49609577655792236\n",
      "0.48294585943222046\n",
      "0.47704827785491943\n",
      "0.4662506580352783\n",
      "0.46100082993507385\n",
      "0.45196497440338135\n",
      "0.44720444083213806\n",
      "0.43952685594558716\n",
      "0.4351608157157898\n",
      "0.42855754494667053\n",
      "0.42452508211135864\n",
      "0.41878876090049744\n",
      "0.4150490462779999\n",
      "0.41002368927001953\n",
      "0.4065464437007904\n",
      "0.402111291885376\n",
      "0.39887332916259766\n",
      "0.3949333727359772\n",
      "0.39191585779190063\n",
      "0.38839489221572876\n",
      "0.38558223843574524\n",
      "0.38241851329803467\n",
      "0.37979644536972046\n",
      "0.37693947553634644\n",
      "0.37449532747268677\n",
      "0.3719031810760498\n",
      "0.36962535977363586\n",
      "0.3672631084918976\n",
      "0.36513999104499817\n",
      "0.36297839879989624\n",
      "0.3609990179538727\n",
      "0.3590128719806671\n",
      "0.35716670751571655\n",
      "0.35533443093299866\n",
      "0.3536110520362854\n",
      "0.3519141376018524\n",
      "0.3503030836582184\n",
      "0.34872564673423767\n",
      "0.3472171425819397\n",
      "0.3457452356815338\n",
      "0.34432992339134216\n",
      "0.3429512083530426\n",
      "0.34162020683288574\n",
      "0.3403245806694031\n",
      "0.33906957507133484\n",
      "0.33784788846969604\n",
      "0.33666175603866577\n",
      "0.33550649881362915\n",
      "0.33438271284103394\n",
      "0.3332873284816742\n",
      "0.3322201371192932\n",
      "0.3311792016029358\n",
      "0.33016377687454224\n",
      "0.32917261123657227\n",
      "0.3282046318054199\n",
      "0.3272591531276703\n",
      "0.32633519172668457\n",
      "0.3254319727420807\n",
      "0.3245488107204437\n",
      "0.3236849308013916\n",
      "0.3228396773338318\n",
      "0.3220124840736389\n",
      "0.32120275497436523\n",
      "0.320409893989563\n",
      "0.3196335434913635\n",
      "0.31887295842170715\n",
      "0.31812790036201477\n",
      "0.31739771366119385\n",
      "0.3166820704936981\n",
      "0.31598055362701416\n",
      "0.31529274582862854\n",
      "0.31461820006370544\n",
      "0.31395667791366577\n",
      "0.3133077323436737\n",
      "0.3126709759235382\n",
      "0.31204620003700256\n",
      "0.31143301725387573\n",
      "0.31083112955093384\n",
      "0.3102401793003082\n",
      "0.30966001749038696\n",
      "0.30909019708633423\n",
      "0.3085305690765381\n",
      "0.3079809248447418\n",
      "0.30744078755378723\n",
      "0.3069101572036743\n",
      "0.3063886761665344\n",
      "0.30587613582611084\n",
      "0.30537232756614685\n",
      "0.30487698316574097\n",
      "0.304390013217926\n",
      "0.30391114950180054\n",
      "0.3034402132034302\n",
      "0.3029769957065582\n",
      "0.302521288394928\n",
      "0.30207300186157227\n",
      "0.3016319274902344\n",
      "0.3011978566646576\n",
      "0.30077067017555237\n",
      "0.3003501296043396\n",
      "0.2999362051486969\n",
      "0.29952871799468994\n",
      "0.2991274893283844\n",
      "0.2987324297428131\n",
      "0.29834333062171936\n",
      "0.29796016216278076\n",
      "0.2975826859474182\n",
      "0.2972109019756317\n",
      "0.2968445122241974\n",
      "0.29648357629776\n",
      "0.29612788558006287\n",
      "0.2957773208618164\n",
      "0.29543188214302063\n",
      "0.2950913906097412\n",
      "0.294755756855011\n",
      "0.294424831867218\n",
      "0.29409855604171753\n",
      "0.29377683997154236\n",
      "0.2934596538543701\n",
      "0.2931467294692993\n",
      "0.2928381562232971\n",
      "0.2925337851047516\n",
      "0.29223358631134033\n",
      "0.29193732142448425\n",
      "0.2916451096534729\n",
      "0.29135677218437195\n",
      "0.2910722494125366\n",
      "0.29079142212867737\n",
      "0.2905142605304718\n",
      "0.2902407646179199\n",
      "0.2899707555770874\n",
      "0.28970423340797424\n",
      "0.2894410789012909\n",
      "0.28918132185935974\n",
      "0.28892481327056885\n",
      "0.2886715531349182\n",
      "0.2884214222431183\n",
      "0.2881743907928467\n",
      "0.287930428981781\n",
      "0.2876894772052765\n",
      "0.2874514162540436\n",
      "0.2872162461280823\n",
      "0.2869839370250702\n",
      "0.28675445914268494\n",
      "0.2865276336669922\n",
      "0.2863036096096039\n",
      "0.28608211874961853\n",
      "0.28586331009864807\n",
      "0.28564706444740295\n",
      "0.28543326258659363\n",
      "0.28522202372550964\n",
      "0.2850131392478943\n",
      "0.28480666875839233\n",
      "0.2846025824546814\n",
      "0.2844007611274719\n",
      "0.2842012345790863\n",
      "0.28400397300720215\n",
      "0.2838088870048523\n",
      "0.28361594676971436\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### Entrainement #####################\n",
    "######################################################\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "\n",
    "\n",
    "\n",
    "for k in range(200):\n",
    "\n",
    "    # 1) Forward pass\n",
    "    emb = C[X]                      # [32, 3, 2]\n",
    "    h = torch.tanh(emb.view(32, 6) @ W1 + b1)   # [32, 100]\n",
    "    logits = h @ W2 + b2           # [32, 27]\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    # 2) Backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None              # reset gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 3) Gradient descent\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad    # gradient descent\n",
    "\n",
    "    print(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
