{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df29115-a89e-48d5-97f7-3065f9bbfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6ce3f-2178-4114-acc0-cc20b25554e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lecture de tous les mots\n",
    "words = open('dataset/names.txt', 'r').read().splitlines()\n",
    "words [:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a36898-1c35-4e83-a45d-3a9df2878544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Création des dictionnaires\n",
    "chars = sorted(list(set(''.join(words)))) # estraction de chaque caracère de la liste de mots\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # Création d'un liste en commençant à l'indice 1 ou chaque caractère est associcé à un index\n",
    "stoi['.'] = 0 #jout du caractère \".\" à l'indice 0\n",
    "itos = {i:s for s,i in stoi.items()} # Création d'une liste inverse pour avoir le nombre et la corespondance du caractere\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c390758-3d4d-40f4-a874-530d13b928f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction du dataset\n",
    "block_size = 3 # taille du contexte dans cet exemple \n",
    "\n",
    "X, Y = [], [] #Création de deux liste X pour les entrées et Y pour la sortie attendue\n",
    "\n",
    "for w in words:\n",
    "    #print (w)\n",
    "    context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "    for ch in w + '.': # Pour chauqe catactère du mot\n",
    "\n",
    "        ix = stoi[ch] # On récupère la valeur du nombre correspondant à la lettre\n",
    "        X.append(context) # On ajoute le contexte à la liste X\n",
    "        Y.append(ix) # On ajoute la valeur numérique du caractère à la liste Y\n",
    "        \n",
    "        #On affiche le contexte + ----> + le caractère du mot que l'on traite \n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        # La même chose mais on affiche les indexes\n",
    "            #print(''.join(str(i) for i in context), '--->', ix)\n",
    " \n",
    "        \n",
    "        \n",
    "        # On jette le premier élément du contexte et on d&calle tout a gauche\n",
    "        #et on ajoute la valeur numérique de la lettre dans le contexte\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "n_lignes_exemples = X.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec19080-9bac-4cc5-b7fe-cb6a28393e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a73b41e-d517-43af-878f-dd0c7e0ff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de la matrice des embeddings, ou chaque caractère est \n",
    "# représenté par une matrice a deux dimensions aléaroires pour commencer\n",
    "C = torch.randn((27, 2))\n",
    "#print (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16eae17-a2a4-4719-b13a-a5cdacf0301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "####### CONSTRUCTION DE LA COUCHE D'EMBEDDING ########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ba1ae9-523d-4ff3-b9d9-a739a36c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding de la matric X\n",
    "# A ce stade X contient les contexte de 3 index\n",
    "# Le réseau ne peut pas travailler avec ces indexes c'est pourquoi on va \n",
    "# embedder X pour donner un représentation mathématique des caractères via\n",
    "# des vecteurs denses\n",
    "# Avant embedding = X[i] = [5, 13, 13]\n",
    "# Après embedding = X[i] = [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# [\n",
    "      #C[5],    # embedding de 'e' → [1.7, -0.3]\n",
    "      #C[13],   # embedding de 'm' → [0.2, +1.5]\n",
    "      #C[13]    # embedding de 'm' → [0.2, +1.5]\n",
    "# ]\n",
    "emb = C[X]\n",
    "# On obtient un tenseur de 32 lignes (le nombre d'exemple dans notre training set), dans chaque ligne 3 éléments (contexte) \n",
    "# et pour chaque élément la version embedder donc une matrice de 2 dimensions\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb97c7e3-9f10-41e2-94a3-8c6fc98eff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######### CONSTRUCTION DE LA HIDDEN LAYER ############\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a78f30e-d7e3-43a0-900e-3160e368ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape emb : torch.Size([228146, 3, 2])\n",
      "shape W1 : torch.Size([6, 100])\n",
      "shape b1 : torch.Size([100])\n",
      "shape emb.view(-1, 6) : torch.Size([228146, 6])\n",
      "shape de h : torch.Size([228146, 100])\n"
     ]
    }
   ],
   "source": [
    "#Initialisatin des poids aléatoirement\n",
    "W1 = torch.randn((6, 100)) \n",
    "# 6 lignes car \n",
    "# 6 entrées car notre embedding contient 6 valeurs par ligne\n",
    "# 3 lettres de contexte * 2 embedding par lettre  \n",
    "# [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# La matrice est applatit avant d'être envoyé au neuronne pour ne faire q'un seul vecteur\n",
    "# [1.7, -0.3, 0.2, 1.5, 0.2, 1.5]   # vecteur de taille 6\n",
    "\n",
    "# 100 neuronnes cachés avec 6 poids => Taille initié à l'expérience :) \n",
    "\n",
    "#Initialisation des biais aléatoirement\n",
    "b1 = torch.randn(100)\n",
    "# 100 biais car 100 neuronnes\n",
    "#Chaque neuronne reçoit la valeur de l'embedding applati,\n",
    "\n",
    "\n",
    "# Il fait son calcul tanh(aX + b) et en sortie on a une valeur\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "\n",
    "#Explication du emb.view\n",
    "# A ce stade on veut muliplier une matrice emb de dimensions [32, 3, 2]\n",
    "# par la matrice W1 de dimension [6, 100]\n",
    "# Ce qui est imossible, la multiplication d'une matrice A par une matrice B impose\n",
    "# que le nombre de colonne de la matrice A soit égal au nombre de ligne de la matrice B\n",
    "# Il faut donc que la matrice emb ai 6 colonne (actuellement 3)\n",
    "# la fonction view permet, sans créer de nouveau tenseur en mémoire, de transoformer \n",
    "# la forme de notre matrice et pour garder une forme dynamique on indique -1 au niveau\n",
    "# de la ligne, torch sait alors que ce sera forcément 32 dans notre cas car il fait\n",
    "# autmatiquement le calcul en fonction de la taille du tenseur d'origine\n",
    "# origine 32 * 3 * 2 = 192 donc si on indique 6 il fait 192 / 6 pour déduire 32\n",
    "\n",
    "print('shape emb :', emb.shape)\n",
    "print('shape W1 :', W1.shape)\n",
    "print('shape b1 :', b1.shape)\n",
    "print('shape emb.view(-1, 6) :', emb.view(-1, 6).shape)\n",
    "\n",
    "# La shape de h sera égale au nombre de ligne de la matrice emb.view(-1, 6) >> 32\n",
    "# Et du nombre de colonne de la matrice W1 >> 100\n",
    "# Règle de la multiplication des matrices\n",
    "\n",
    "##  Règle mathématique d'addition de matrice, pour addition deux matrices il faut qu'elle soient de mêm taille :\n",
    "# L'addition de emb.view(-1, 6) @ W1  b1 à 1 dimension  est possible grace au\n",
    "# Broadcasting, qui va étendre automatiquement le plus petit vecteur pour que la forme soit compatible\n",
    "# b1 est un vecteru composé d'une ligne et de 100 valeurs (colonnes)\n",
    "# Pour permettre l'addition le broadcasting va répété virtuellement b1 sur 32 lignes\n",
    "print('shape de h :', h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8613d0-02a1-4f23-9ba2-0caa815583a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######## CONSTRUCTION DE LA COUCHE DE SORTIE #########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21366d7-6e33-4238-ba11-b4b003eb6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisatin des poids aléatoirement\n",
    "W2 = torch.randn((100, 27)) \n",
    "# 100 lignes car \n",
    "# 100 sorties de notre hidden layer (shape de h : torch.Size([32, 100]))\n",
    "# 27 colonnes car 27 sortie étant données le nombre de caractère possible   \n",
    "\n",
    "#Initialisation des biais aléatoirement\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db8231-d0b2-4a22-bf49-0d21e96dfed9",
   "metadata": {},
   "source": [
    "#### 1. 27 Scores bruts non normalisés, un niveau \"d'energie attribué a chaque caractère\"\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "#### On va calculer la probabilité\n",
    "    # counts = logits.exp() # Transforme chaque score brut en nombre strictement positif \n",
    "    # on le divise par la valeur des sommes de toutes les colonnes pour en faire une moyenne »\n",
    "    # prob = counts / counts.sum(1, keepdims=True) ## Normalisation pour que la somme d'une ligne soit égale à 1\n",
    "##### prob contient 32 lignes (nombre d'exemple) et 27 colonne représentant chauqe caractère et la probabilité affecté a chaque caractère\n",
    "##### Création d'un vecteur de 32 valeurs dans le quel on stock la valeur prédite par le modèle\n",
    "##### Pour chaque caractère qui était attendu (donc ce n'est forcément celui qui a eu la meilleur probabilité\n",
    "##### Ce qui est normal, à ce stade le modèle n'est pas encore entrainé)\n",
    "    # preparationCalculLoss = prob[torch.arange(32), Y]\n",
    "    # print (preparationCalculLoss)\n",
    "\n",
    "##### On prend la probabilité du caractère correct, on prend son log, on met un signe moins, on moyenne :\n",
    "##### c’est la Negative Log-Likelihood, la mesure standard de “à quel point le modèle a eu tort”.\n",
    "##### C'est cette valeur que l'on va tenter de minimiser pour améliorer notre modèle\n",
    "    # loss = -preparationCalculLoss.log().mean()\n",
    "    # print (loss.item())\n",
    "\n",
    "#### La fonction F.cross_entropy(logits, y) faile calcul du neagative loglikelihood de façon plus efficiente\n",
    "#### cross_entropy fait tout en une seule opération efficace au lieu de 4 quand on le fait manuellement\n",
    "#### cross_entropy est NUMÉRIQUEMENT STABLE >> \n",
    "    # Si un logit = 50 → exp(50) ≈ 5 × 10²¹ → explosion\n",
    "    # Si un logit = –50 → exp(–50) ≈ 1.9e–22 → underflow\n",
    "        # ➡️ aucun overflow\n",
    "        # ➡️ aucun underflow\n",
    "        # ➡️ même si les logits sont énormes\n",
    "        # ➡️ gradients fiables\n",
    "#### C’est plus rapide (implémenté en C++/CUDA)\n",
    "     # loss = F.cross_entropy(logits, Y)\n",
    "     # print('Caclul de la loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68bab2c-6ca6-4153-8b59-a6200f81c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [C, W1, W2, b1, b2]\n",
    "# Si on ne fait pas ça on a cette erreur\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41779593-87bc-4b01-83a7-af24d27226fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre modèle compte  3481  paramètres\n"
     ]
    }
   ],
   "source": [
    "# Afficher le nombre de paramètre\n",
    "print('Notre modèle compte ',sum(p.nelement() for p in parameters), ' paramètres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "329784c4-4023-4d79-b108-ff3d0e7dd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9489023685455322\n",
      "2.3617706298828125\n",
      "2.889514446258545\n",
      "2.5898845195770264\n",
      "2.6945488452911377\n",
      "2.844996690750122\n",
      "2.4480977058410645\n",
      "2.79352068901062\n",
      "2.9480772018432617\n",
      "2.5860331058502197\n",
      "2.4731650352478027\n",
      "2.9381203651428223\n",
      "2.8796446323394775\n",
      "2.6268653869628906\n",
      "2.651540517807007\n",
      "2.387712240219116\n",
      "2.427774429321289\n",
      "2.952765941619873\n",
      "2.5663909912109375\n",
      "2.5221221446990967\n",
      "2.911930799484253\n",
      "2.587578296661377\n",
      "2.6222071647644043\n",
      "2.7535994052886963\n",
      "2.8521504402160645\n",
      "2.692403554916382\n",
      "2.588111162185669\n",
      "2.836526393890381\n",
      "2.412205457687378\n",
      "2.3368563652038574\n",
      "2.4133541584014893\n",
      "2.6689376831054688\n",
      "2.5828773975372314\n",
      "2.338578939437866\n",
      "2.755924701690674\n",
      "2.7723278999328613\n",
      "2.599684715270996\n",
      "2.611922025680542\n",
      "2.9949164390563965\n",
      "2.6815855503082275\n",
      "2.5212833881378174\n",
      "2.510392189025879\n",
      "2.539912462234497\n",
      "2.8395440578460693\n",
      "2.69075345993042\n",
      "2.7477071285247803\n",
      "2.986111879348755\n",
      "2.7057700157165527\n",
      "2.44169020652771\n",
      "2.577143907546997\n",
      "2.237252950668335\n",
      "2.298265218734741\n",
      "2.2894606590270996\n",
      "2.5095815658569336\n",
      "2.7628605365753174\n",
      "2.662130117416382\n",
      "2.6521308422088623\n",
      "3.0324440002441406\n",
      "2.7175753116607666\n",
      "2.8784775733947754\n",
      "3.1569793224334717\n",
      "2.8354880809783936\n",
      "2.7504642009735107\n",
      "2.9087893962860107\n",
      "2.50546932220459\n",
      "2.648606538772583\n",
      "2.767756700515747\n",
      "2.6066277027130127\n",
      "2.668041229248047\n",
      "2.922487497329712\n",
      "2.5977511405944824\n",
      "2.825514793395996\n",
      "2.7888357639312744\n",
      "2.723111152648926\n",
      "2.727285623550415\n",
      "3.0641701221466064\n",
      "2.8940634727478027\n",
      "2.52848219871521\n",
      "2.8968279361724854\n",
      "2.8773226737976074\n",
      "2.3295493125915527\n",
      "2.719442129135132\n",
      "2.526177406311035\n",
      "2.5033116340637207\n",
      "2.4303081035614014\n",
      "2.969374418258667\n",
      "2.7204318046569824\n",
      "2.798036575317383\n",
      "2.518378973007202\n",
      "2.9326343536376953\n",
      "2.4537975788116455\n",
      "2.417262315750122\n",
      "2.496814727783203\n",
      "2.947944402694702\n",
      "2.6782126426696777\n",
      "2.2592039108276367\n",
      "2.4784185886383057\n",
      "3.132248640060425\n",
      "2.342581272125244\n",
      "2.875291585922241\n",
      "2.652116537094116\n",
      "2.8423759937286377\n",
      "2.4237489700317383\n",
      "2.8247740268707275\n",
      "2.646688938140869\n",
      "2.8378407955169678\n",
      "2.67663836479187\n",
      "2.5811779499053955\n",
      "2.8448266983032227\n",
      "2.5533320903778076\n",
      "2.715897798538208\n",
      "2.8566436767578125\n",
      "2.498903274536133\n",
      "2.5050525665283203\n",
      "2.8120219707489014\n",
      "2.2480170726776123\n",
      "2.54662823677063\n",
      "2.492973804473877\n",
      "2.9044063091278076\n",
      "2.3695340156555176\n",
      "2.59031081199646\n",
      "2.460681915283203\n",
      "2.9960598945617676\n",
      "2.4911086559295654\n",
      "2.6505355834960938\n",
      "2.8107528686523438\n",
      "3.0207440853118896\n",
      "2.571880578994751\n",
      "2.786318778991699\n",
      "2.4934775829315186\n",
      "2.5941455364227295\n",
      "3.0250606536865234\n",
      "2.6507420539855957\n",
      "2.5300819873809814\n",
      "2.782235860824585\n",
      "3.0811829566955566\n",
      "2.391530752182007\n",
      "2.400766372680664\n",
      "2.442406177520752\n",
      "2.5593419075012207\n",
      "2.1158828735351562\n",
      "2.6264827251434326\n",
      "2.7760634422302246\n",
      "2.834726095199585\n",
      "2.802137613296509\n",
      "2.353314161300659\n",
      "2.564117670059204\n",
      "2.5321967601776123\n",
      "2.596816062927246\n",
      "2.5710232257843018\n",
      "2.276348352432251\n",
      "2.720939874649048\n",
      "2.853485107421875\n",
      "2.510283946990967\n",
      "2.930588483810425\n",
      "2.727652072906494\n",
      "2.4546194076538086\n",
      "2.503206491470337\n",
      "3.125103712081909\n",
      "2.5819661617279053\n",
      "2.7384822368621826\n",
      "2.5981462001800537\n",
      "2.549304962158203\n",
      "2.653972625732422\n",
      "2.774994373321533\n",
      "2.555546522140503\n",
      "2.652017831802368\n",
      "2.406914472579956\n",
      "2.449599266052246\n",
      "2.732114791870117\n",
      "2.553680181503296\n",
      "2.4894814491271973\n",
      "2.4870119094848633\n",
      "2.4750638008117676\n",
      "2.575469732284546\n",
      "2.8479785919189453\n",
      "2.4915449619293213\n",
      "2.5051374435424805\n",
      "2.337523937225342\n",
      "2.886404037475586\n",
      "2.5168237686157227\n",
      "2.679405689239502\n",
      "2.8264472484588623\n",
      "2.875067710876465\n",
      "2.790151596069336\n",
      "2.920579195022583\n",
      "2.5382895469665527\n",
      "2.790766716003418\n",
      "2.776031970977783\n",
      "2.669663667678833\n",
      "2.28937029838562\n",
      "2.7096309661865234\n",
      "2.757249593734741\n",
      "2.757967710494995\n",
      "2.5229103565216064\n",
      "2.776685953140259\n",
      "2.8404133319854736\n",
      "2.619943618774414\n",
      "2.835749864578247\n",
      "2.8361480236053467\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### Entrainement #####################\n",
    "######################################################\n",
    "for k in range(200): # Sur deux cents itérations\n",
    "\n",
    "    #Construction d'un minibach\n",
    "    # Le but ici est d'accélerer l'entrainement et de consommer moins de RAM et de CPU\n",
    "    # Sans le batch, on fait entraine le réseau sur tout le dataset d'un coup\n",
    "    # Donc on fait plus de 220k forward et backward avec toutes les données en même temps\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) #Cette ligne tire 32 indices aléatoires dans le dataset pour construire un mini-batch.\n",
    "    \n",
    "    # 1) Forward pass\n",
    "    emb = C[X[ix]]    \n",
    "    #Cette ligne remplace les indices des caractères du mini-batch par leur embedding dense, \n",
    "    #créant un tenseur [32,3,2] prêt pour la couche cachée.\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)   # Hidden layer [32, 100]\n",
    "    logits = h @ W2 + b2           # Couche de sortie [32, 27] \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # Calcul de la loss (exp + counts / counts.sum ...) \n",
    "    print(loss.item())\n",
    "    # 2) Backprop\n",
    "    # Les gradients sont générés et stockés automatiquement par loss.backward() \n",
    "    # et on les efface manuellement juste après les avoir utilisés pour mettre à jour les poids\n",
    "    for p in parameters:\n",
    "        p.grad = None  \n",
    "    \n",
    "    loss.backward() \n",
    "\n",
    "    # 3) Gradient descent >> Mise à jour des poids\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad    # descente de gradiant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7b385-cdcc-448d-a168-3c302284dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprendre a 45 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
