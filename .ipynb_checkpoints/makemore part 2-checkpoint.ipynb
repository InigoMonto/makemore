{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df29115-a89e-48d5-97f7-3065f9bbfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6ce3f-2178-4114-acc0-cc20b25554e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lecture de tous les mots\n",
    "words = open('dataset/names.txt', 'r').read().splitlines()\n",
    "words [:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a36898-1c35-4e83-a45d-3a9df2878544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Cr√©ation des dictionnaires\n",
    "chars = sorted(list(set(''.join(words)))) # estraction de chaque carac√®re de la liste de mots\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # Cr√©ation d'un liste en commen√ßant √† l'indice 1 ou chaque caract√®re est associc√© √† un index\n",
    "stoi['.'] = 0 #jout du caract√®re \".\" √† l'indice 0\n",
    "itos = {i:s for s,i in stoi.items()} # Cr√©ation d'une liste inverse pour avoir le nombre et la corespondance du caractere\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c390758-3d4d-40f4-a874-530d13b928f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction du dataset\n",
    "block_size = 3 # taille du contexte dans cet exemple \n",
    "\n",
    "X, Y = [], [] #Cr√©ation de deux liste X pour les entr√©es et Y pour la sortie attendue\n",
    "\n",
    "for w in words:\n",
    "    #print (w)\n",
    "    context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "    for ch in w + '.': # Pour chauqe catact√®re du mot\n",
    "\n",
    "        ix = stoi[ch] # On r√©cup√®re la valeur du nombre correspondant √† la lettre\n",
    "        X.append(context) # On ajoute le contexte √† la liste X\n",
    "        Y.append(ix) # On ajoute la valeur num√©rique du caract√®re √† la liste Y\n",
    "        \n",
    "        #On affiche le contexte + ----> + le caract√®re du mot que l'on traite \n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        # La m√™me chose mais on affiche les indexes\n",
    "            #print(''.join(str(i) for i in context), '--->', ix)\n",
    " \n",
    "        \n",
    "        \n",
    "        # On jette le premier √©l√©ment du contexte et on d&calle tout a gauche\n",
    "        #et on ajoute la valeur num√©rique de la lettre dans le contexte\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "n_lignes_exemples = X.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec19080-9bac-4cc5-b7fe-cb6a28393e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a73b41e-d517-43af-878f-dd0c7e0ff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cr√©ation de la matrice des embeddings, ou chaque caract√®re est \n",
    "# repr√©sent√© par une matrice a deux dimensions al√©aroires pour commencer\n",
    "C = torch.randn((27, 2))\n",
    "#print (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16eae17-a2a4-4719-b13a-a5cdacf0301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "####### CONSTRUCTION DE LA COUCHE D'EMBEDDING ########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ba1ae9-523d-4ff3-b9d9-a739a36c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding de la matric X\n",
    "# A ce stade X contient les contexte de 3 index\n",
    "# Le r√©seau ne peut pas travailler avec ces indexes c'est pourquoi on va \n",
    "# embedder X pour donner un repr√©sentation math√©matique des caract√®res via\n",
    "# des vecteurs denses\n",
    "# Avant embedding = X[i] = [5, 13, 13]\n",
    "# Apr√®s embedding = X[i] = [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# [\n",
    "      #C[5],    # embedding de 'e' ‚Üí [1.7, -0.3]\n",
    "      #C[13],   # embedding de 'm' ‚Üí [0.2, +1.5]\n",
    "      #C[13]    # embedding de 'm' ‚Üí [0.2, +1.5]\n",
    "# ]\n",
    "emb = C[X]\n",
    "# On obtient un tenseur de 32 lignes (le nombre d'exemple dans notre training set), dans chaque ligne 3 √©l√©ments (contexte) \n",
    "# et pour chaque √©l√©ment la version embedder donc une matrice de 2 dimensions\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb97c7e3-9f10-41e2-94a3-8c6fc98eff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######### CONSTRUCTION DE LA HIDDEN LAYER ############\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a78f30e-d7e3-43a0-900e-3160e368ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape emb : torch.Size([228146, 3, 2])\n",
      "shape W1 : torch.Size([6, 100])\n",
      "shape b1 : torch.Size([100])\n",
      "shape emb.view(-1, 6) : torch.Size([228146, 6])\n",
      "shape de h : torch.Size([228146, 100])\n"
     ]
    }
   ],
   "source": [
    "#Initialisatin des poids al√©atoirement\n",
    "W1 = torch.randn((6, 100)) \n",
    "# 6 lignes car \n",
    "# 6 entr√©es car notre embedding contient 6 valeurs par ligne\n",
    "# 3 lettres de contexte * 2 embedding par lettre  \n",
    "# [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# La matrice est applatit avant d'√™tre envoy√© au neuronne pour ne faire q'un seul vecteur\n",
    "# [1.7, -0.3, 0.2, 1.5, 0.2, 1.5]   # vecteur de taille 6\n",
    "\n",
    "# 100 neuronnes cach√©s avec 6 poids => Taille initi√© √† l'exp√©rience :) \n",
    "\n",
    "#Initialisation des biais al√©atoirement\n",
    "b1 = torch.randn(100)\n",
    "# 100 biais car 100 neuronnes\n",
    "#Chaque neuronne re√ßoit la valeur de l'embedding applati,\n",
    "\n",
    "\n",
    "# Il fait son calcul tanh(aX + b) et en sortie on a une valeur\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "\n",
    "#Explication du emb.view\n",
    "# A ce stade on veut muliplier une matrice emb de dimensions [32, 3, 2]\n",
    "# par la matrice W1 de dimension [6, 100]\n",
    "# Ce qui est imossible, la multiplication d'une matrice A par une matrice B impose\n",
    "# que le nombre de colonne de la matrice A soit √©gal au nombre de ligne de la matrice B\n",
    "# Il faut donc que la matrice emb ai 6 colonne (actuellement 3)\n",
    "# la fonction view permet, sans cr√©er de nouveau tenseur en m√©moire, de transoformer \n",
    "# la forme de notre matrice et pour garder une forme dynamique on indique -1 au niveau\n",
    "# de la ligne, torch sait alors que ce sera forc√©ment 32 dans notre cas car il fait\n",
    "# autmatiquement le calcul en fonction de la taille du tenseur d'origine\n",
    "# origine 32 * 3 * 2 = 192 donc si on indique 6 il fait 192 / 6 pour d√©duire 32\n",
    "\n",
    "print('shape emb :', emb.shape)\n",
    "print('shape W1 :', W1.shape)\n",
    "print('shape b1 :', b1.shape)\n",
    "print('shape emb.view(-1, 6) :', emb.view(-1, 6).shape)\n",
    "\n",
    "# La shape de h sera √©gale au nombre de ligne de la matrice emb.view(-1, 6) >> 32\n",
    "# Et du nombre de colonne de la matrice W1 >> 100\n",
    "# R√®gle de la multiplication des matrices\n",
    "\n",
    "##  R√®gle math√©matique d'addition de matrice, pour addition deux matrices il faut qu'elle soient de m√™m taille :\n",
    "# L'addition de emb.view(-1, 6) @ W1  b1 √† 1 dimension  est possible grace au\n",
    "# Broadcasting, qui va √©tendre automatiquement le plus petit vecteur pour que la forme soit compatible\n",
    "# b1 est un vecteru compos√© d'une ligne et de 100 valeurs (colonnes)\n",
    "# Pour permettre l'addition le broadcasting va r√©p√©t√© virtuellement b1 sur 32 lignes\n",
    "print('shape de h :', h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8613d0-02a1-4f23-9ba2-0caa815583a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######## CONSTRUCTION DE LA COUCHE DE SORTIE #########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21366d7-6e33-4238-ba11-b4b003eb6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisatin des poids al√©atoirement\n",
    "W2 = torch.randn((100, 27)) \n",
    "# 100 lignes car \n",
    "# 100 sorties de notre hidden layer (shape de h : torch.Size([32, 100]))\n",
    "# 27 colonnes car 27 sortie √©tant donn√©es le nombre de caract√®re possible   \n",
    "\n",
    "#Initialisation des biais al√©atoirement\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db8231-d0b2-4a22-bf49-0d21e96dfed9",
   "metadata": {},
   "source": [
    "#### 1. 27 Scores bruts non normalis√©s, un niveau \"d'energie attribu√© a chaque caract√®re\"\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "#### On va calculer la probabilit√©\n",
    "    # counts = logits.exp() # Transforme chaque score brut en nombre strictement positif \n",
    "    # on le divise par la valeur des sommes de toutes les colonnes pour en faire une moyenne ¬ª\n",
    "    # prob = counts / counts.sum(1, keepdims=True) ## Normalisation pour que la somme d'une ligne soit √©gale √† 1\n",
    "##### prob contient 32 lignes (nombre d'exemple) et 27 colonne repr√©sentant chauqe caract√®re et la probabilit√© affect√© a chaque caract√®re\n",
    "##### Cr√©ation d'un vecteur de 32 valeurs dans le quel on stock la valeur pr√©dite par le mod√®le\n",
    "##### Pour chaque caract√®re qui √©tait attendu (donc ce n'est forc√©ment celui qui a eu la meilleur probabilit√©\n",
    "##### Ce qui est normal, √† ce stade le mod√®le n'est pas encore entrain√©)\n",
    "    # preparationCalculLoss = prob[torch.arange(32), Y]\n",
    "    # print (preparationCalculLoss)\n",
    "\n",
    "##### On prend la probabilit√© du caract√®re correct, on prend son log, on met un signe moins, on moyenne :\n",
    "##### c‚Äôest la Negative Log-Likelihood, la mesure standard de ‚Äú√† quel point le mod√®le a eu tort‚Äù.\n",
    "##### C'est cette valeur que l'on va tenter de minimiser pour am√©liorer notre mod√®le\n",
    "    # loss = -preparationCalculLoss.log().mean()\n",
    "    # print (loss.item())\n",
    "\n",
    "#### La fonction F.cross_entropy(logits, y) faile calcul du neagative loglikelihood de fa√ßon plus efficiente\n",
    "#### cross_entropy fait tout en une seule op√©ration efficace au lieu de 4 quand on le fait manuellement\n",
    "#### cross_entropy est NUM√âRIQUEMENT STABLE >> \n",
    "    # Si un logit = 50 ‚Üí exp(50) ‚âà 5 √ó 10¬≤¬π ‚Üí explosion\n",
    "    # Si un logit = ‚Äì50 ‚Üí exp(‚Äì50) ‚âà 1.9e‚Äì22 ‚Üí underflow\n",
    "        # ‚û°Ô∏è aucun overflow\n",
    "        # ‚û°Ô∏è aucun underflow\n",
    "        # ‚û°Ô∏è m√™me si les logits sont √©normes\n",
    "        # ‚û°Ô∏è gradients fiables\n",
    "#### C‚Äôest plus rapide (impl√©ment√© en C++/CUDA)\n",
    "     # loss = F.cross_entropy(logits, Y)\n",
    "     # print('Caclul de la loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88020f6-1fc4-4901-8c09-747c0888ae3d",
   "metadata": {},
   "source": [
    "######################################################\n",
    "################### Entrainement #####################\n",
    "######################################################\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "# Si on ne fait pas √ßa on a cette erreur\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Afficher le nombre de param√®tre\n",
    "print('Notre mod√®le compte ',sum(p.nelement() for p in parameters), ' param√®tres')\n",
    "\n",
    "for k in range(10000): # Sur deux cents it√©rations\n",
    "\n",
    "    #Construction d'un minibach\n",
    "    # Le but ici est d'acc√©lerer l'entrainement et de consommer moins de RAM et de CPU\n",
    "    # Sans le batch, on fait entraine le r√©seau sur tout le dataset d'un coup\n",
    "    # Donc on fait plus de 220k forward et backward avec toutes les donn√©es en m√™me temps\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) #Cette ligne tire 32 indices al√©atoires dans le dataset pour construire un mini-batch.\n",
    "\n",
    "    # Remarque : \n",
    "            # ce batching implique parfois la r√©utilisatin du m√™me exemple\n",
    "            # Certains exemples ne seront pas utilis√©s (environ 40% sur des batch de 32 exemples dans 200 it√©rations)\n",
    "            # Malgr√©s cela la descente de gradiant reste bonne\n",
    "            # C'est la technique utilis√© par les LLMs comme Llama ou Gemini\n",
    "    \n",
    "    # 1) Forward pass\n",
    "    emb = C[X[ix]]    \n",
    "    #Cette ligne remplace les indices des caract√®res du mini-batch par leur embedding dense, \n",
    "    #cr√©ant un tenseur [32,3,2] pr√™t pour la couche cach√©e.\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)   # Hidden layer [32, 100]\n",
    "    logits = h @ W2 + b2           # Couche de sortie [32, 27] \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # Calcul de la loss (exp + counts / counts.sum ...) \n",
    "\n",
    "    # 2) Backprop\n",
    "    # Les gradients sont g√©n√©r√©s et stock√©s automatiquement par loss.backward() \n",
    "    # et on les efface manuellement juste apr√®s les avoir utilis√©s pour mettre √† jour les poids\n",
    "    for p in parameters:\n",
    "        p.grad = None  \n",
    "    \n",
    "    loss.backward() \n",
    "\n",
    "    # 3) Gradient descent >> Mise √† jour des poids\n",
    "    for p in parameters:\n",
    "        p.data += -0.001 * p.grad    # descente de gradiant\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e4bf0-bef6-495c-9b37-ef1dd9737020",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">üî• Note sur la d√©finition du learning rate</h2>\n",
    "\n",
    "Pour trouver une bonne valeur du learning rate, La m√©thode utilis√©e en deep learning moderne :\n",
    "\n",
    "On teste plusieurs learning rates :\n",
    "par ex. [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "On lance quelques it√©rations pour chaque valeur\n",
    "\n",
    "On observe la courbe de loss\n",
    "\n",
    "‚úî Le bon learning rate est : \n",
    "celui o√π la loss baisse rapidement sans instabilit√© et sans oscillations violentes\n",
    "\n",
    "### üî• Qu‚Äôest-ce que torch.linspace ?\n",
    "\n",
    "#torch.linspace(start, end, steps) cr√©e un vecteur de valeurs r√©guli√®rement espac√©es entre start et end.\n",
    "torch.linspace(0, 1, 5)\n",
    "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]) >> 5 valeurs √©galement esp√©c√©es entre 0 et 1\n",
    "\n",
    "Dans notre exp√©rimentation, on va tester plusieurs valeurs possibles du learning rate.\n",
    " Le principe :\n",
    " - On boucle sur ~100 it√©rations d'entra√Ænement pour chaque learning rate test√©\n",
    " - √Ä chaque essai, on enregistre : \n",
    "       ‚Ä¢ la valeur du learning rate utilis√©\n",
    "       ‚Ä¢ la loss obtenue apr√®s ces it√©rations\n",
    " - Ensuite, on trace une courbe (loss en fonction du learning rate)\n",
    "   afin d‚Äôidentifier la valeur qui permet au mod√®le d‚Äôapprendre le plus efficacement.\n",
    "\n",
    " Cette m√©thode est une pratique courante en deep learning : elle permet de choisir\n",
    " un learning rate qui fait descendre la loss rapidement, sans instabilit√©s.\n",
    "\n",
    "Une fois le learning rate optimal identifi√©, on entra√Æne le mod√®le avec cette valeur jusqu‚Äô√† atteindre un palier (plateau de loss).\n",
    "Pour affiner encore la convergence, on r√©duit ensuite le learning rate d‚Äôun facteur 10 et on continue l‚Äôentra√Ænement : cela permet souvent de diminuer la loss un peu plus finement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adcc719-b2d5-4594-aada-c9ce11c697da",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">üî• Le standart dans les entrainements de mod√®le est de d√©couper le dataset en 3 parties\n",
    "</h2>\n",
    "\n",
    "#### ---> La partie entrainement (80% des donn√©es)\n",
    " ####     -  pour entrainer les param√®tres du mod√®le C, W1, W2, b1, b2\n",
    "#### ---> La partie dev / validation (10% des donn√©es)\n",
    " ####     -pour tester les hyperparam√®tres learning rate, taille du batch, dimensions de embeddings, taille du hiden                layer, architecture\n",
    "#### ---> La partie test (10% des donn√©es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27f2b1b5-7732-418d-9021-9f93d18aa8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626\n",
      "28829\n",
      "dataset d'entrainement :  182511  trigrammes\n",
      "dataset de dev / Validation :  22834  trigrammes\n",
      "dataset de test :  22801  trigrammes\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "## Cr√©ation des datasets d'entainement/ dev / test ###\n",
    "######################################################\n",
    "\n",
    "# Cr√©ation d'une fonction r√©utilisatble\n",
    "def build_dataset(words):\n",
    "    block_size = 3 # taille du contexte dans cet exemple \n",
    "    \n",
    "    X, Y = [], [] #Cr√©ation de deux liste X pour les entr√©es et Y pour la sortie attendue\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "        for ch in w + '.': # Pour chauqe catact√®re du mot\n",
    "    \n",
    "            ix = stoi[ch] # On r√©cup√®re la valeur du nombre correspondant √† la lettre\n",
    "            X.append(context) # On ajoute le contexte √† la liste X\n",
    "            Y.append(ix) # On ajoute la valeur num√©rique du caract√®re √† la liste Y\n",
    "            \n",
    "            # On jette le premier √©l√©ment du contexte et on d&calle tout a gauche\n",
    "            #et on ajoute la valeur num√©rique de la lettre dans le contexte\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    #print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# Utilisation de la fonction pour cr√©er les diff√©rents dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words) # On m√©lange al√©atoirment la liste de mot\n",
    "n1 = int(0.8*len(words))\n",
    "print(n1)\n",
    "n2 = int(0.9*len(words))\n",
    "print(n2)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # On cr√©er une liste avec les mots compris dans le mot 1 et le mot 25626 (80%)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # On cr√©er une liste avec les mots compris  entre le mot 25626 et le mot 28829 (10%)\n",
    "Xte, Yte = build_dataset(words[n2:]) # On cr√©er une liste avc les mots apr√®s 28829 (10%)\n",
    "\n",
    "print('dataset d\\'entrainement : ', Xtr.shape[0], ' trigrammes')\n",
    "print('dataset de dev / Validation : ', Xdev.shape[0], ' trigrammes')\n",
    "print('dataset de test : ', Xte.shape[0], ' trigrammes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d924cb54-d011-4be0-828c-4f3dcdcf7aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre mod√®le compte  3481  param√®tres\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### PARAM√àTRES #######################\n",
    "######################################################\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2)) # Couche d'embedding\n",
    "W1 = torch.randn((6, 100)) # Poids Hidden Layer\n",
    "b1 = torch.randn(100) # Biais Hidden layer\n",
    "W2 = torch.randn((100, 27)) # Poids sortie\n",
    "b2 = torch.randn(27) # Biais sortie\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "\n",
    "# Afficher le nombre de param√®tre\n",
    "print('Notre mod√®le compte ',sum(p.nelement() for p in parameters), ' param√®tres')\n",
    "\n",
    "# Si on ne fait pas √ßa on a cette erreur\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "lre = torch.linspace (-3, 0, 10000) ##g√©n√®re 1000 valeurs iniform√©ment r√©parties entre -3 et 0\n",
    "#print (lre) # tensor([-3.0000, -2.9970, -2.9940, -2.9910, -2.9880, -2.9850, ...]\n",
    "lrs = 10**lre # Exponentiel en base 10 √† chaque √©l√©ment > Transforme les exposants lre en learning rates\n",
    "# Permte de tester sur diff√©rents ordre de grandeur (0,1 / 0,01 / 0,001 ...)  pliutot que sue un espacement lin√©aire (-3.0000, -2.9970, -2.9940)\n",
    "#print (lrs) # ([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12b8d985-0eb3-481d-b654-3947a101c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Entrainement:  1.8321003913879395\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### ENTRAINEMENT #####################\n",
    "######################################################\n",
    "lri, lossi = [], [] ## Stockage des learning rates essay√© avec les les loss asosci√©s \n",
    "\n",
    "for i in range(10000): \n",
    "\n",
    "    #Construction d'un minibach\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,)) #Cette ligne tire 32 indices al√©atoires dans le dataset pour construire un mini-batch.\n",
    "    \n",
    "    # 1) Forward pass\n",
    "\n",
    "    # 1.1) Embedding\n",
    "    emb = C[Xtr[ix]]    \n",
    "    #Cette ligne remplace les indices des caract√®res du mini-batch par leur embedding dense, \n",
    "    #cr√©ant un tenseur [32,3,2] pr√™t pour la couche cach√©e.\n",
    "\n",
    "    # 1.2) Hidden layer\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)   # Hidden layer [32, 100]\n",
    "\n",
    "    # 1.3) Sortie \n",
    "    logits = h @ W2 + b2           # Couche de sortie [32, 27] \n",
    "    \n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # Calcul de la loss (exp + counts / counts.sum ...) \n",
    "\n",
    "    # 2) Backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None  \n",
    "    loss.backward() \n",
    "\n",
    "    # 3) Mise √† jour des lri et lossi\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.01\n",
    "\n",
    "    # 3) Gradient descent >> Mise √† jour des poids\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad    # descente de gradiant\n",
    "\n",
    "    # Track stats\n",
    "    #lri.append(lre[i])\n",
    "    #lossi.append(loss.item())\n",
    "\n",
    "## Affich√© la courbe avec matplot des learning rates et loss pour voir ou se situe le learnig rate le plus efficace et stable\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.plot(lrs[:len(lossi)], lossi)\n",
    "#plt.xscale('log')\n",
    "#plt.xlabel('Learning rate (log scale)')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.title('LR Finder ‚Äì Evolution de la loss en fonction du learning rate')\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "\n",
    "#print('Loss Entrainement: ',loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "441729aa-01c5-4f05-8af0-ca040994fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Fin entrainement :  2.2157983779907227\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "# 1.2) Hidden layer [32, 100]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)  \n",
    "# 1.3) Sortie [32, 27]\n",
    "logits = h @ W2 + b2      \n",
    "lossEval = F.cross_entropy(logits, Ytr)\n",
    "print('Loss Fin entrainement : ',lossEval.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ce934d6-3c2a-486e-80a6-c1da55a93496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Dev:  2.2366387844085693\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### EVALUATION  ######################\n",
    "######################################################\n",
    "\n",
    "#Maintenant on va √©avaluer le mod√®le avec le datset Xdev\n",
    "# Embedding\n",
    "emb = C[Xdev]\n",
    "# 1.2) Hidden layer [32, 100]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)  \n",
    "# 1.3) Sortie [32, 27]\n",
    "logits = h @ W2 + b2      \n",
    "lossEval = F.cross_entropy(logits, Ydev)\n",
    "print('Loss Dev: ',lossEval.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00847b-e9c5-478c-89da-0d0e9df3460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reprendre la vid√©o √† une 1h00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
