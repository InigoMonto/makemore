{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df29115-a89e-48d5-97f7-3065f9bbfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6ce3f-2178-4114-acc0-cc20b25554e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lecture de tous les mots\n",
    "words = open('dataset/names.txt', 'r').read().splitlines()\n",
    "words [:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a36898-1c35-4e83-a45d-3a9df2878544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Cr√©ation des dictionnaires\n",
    "chars = sorted(list(set(''.join(words)))) # estraction de chaque carac√®re de la liste de mots\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # Cr√©ation d'un liste en commen√ßant √† l'indice 1 ou chaque caract√®re est associc√© √† un index\n",
    "stoi['.'] = 0 #jout du caract√®re \".\" √† l'indice 0\n",
    "itos = {i:s for s,i in stoi.items()} # Cr√©ation d'une liste inverse pour avoir le nombre et la corespondance du caractere\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c390758-3d4d-40f4-a874-530d13b928f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction du dataset\n",
    "block_size = 3 # taille du contexte dans cet exemple \n",
    "\n",
    "X, Y = [], [] #Cr√©ation de deux liste X pour les entr√©es et Y pour la sortie attendue\n",
    "\n",
    "for w in words:\n",
    "    #print (w)\n",
    "    context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "    for ch in w + '.': # Pour chauqe catact√®re du mot\n",
    "\n",
    "        ix = stoi[ch] # On r√©cup√®re la valeur du nombre correspondant √† la lettre\n",
    "        X.append(context) # On ajoute le contexte √† la liste X\n",
    "        Y.append(ix) # On ajoute la valeur num√©rique du caract√®re √† la liste Y\n",
    "        \n",
    "        #On affiche le contexte + ----> + le caract√®re du mot que l'on traite \n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        # La m√™me chose mais on affiche les indexes\n",
    "            #print(''.join(str(i) for i in context), '--->', ix)\n",
    " \n",
    "        \n",
    "        \n",
    "        # On jette le premier √©l√©ment du contexte et on d&calle tout a gauche\n",
    "        #et on ajoute la valeur num√©rique de la lettre dans le contexte\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "n_lignes_exemples = X.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec19080-9bac-4cc5-b7fe-cb6a28393e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a73b41e-d517-43af-878f-dd0c7e0ff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cr√©ation de la matrice des embeddings, ou chaque caract√®re est \n",
    "# repr√©sent√© par une matrice a deux dimensions al√©aroires pour commencer\n",
    "C = torch.randn((27, 2))\n",
    "#print (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16eae17-a2a4-4719-b13a-a5cdacf0301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "####### CONSTRUCTION DE LA COUCHE D'EMBEDDING ########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ba1ae9-523d-4ff3-b9d9-a739a36c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding de la matric X\n",
    "# A ce stade X contient les contexte de 3 index\n",
    "# Le r√©seau ne peut pas travailler avec ces indexes c'est pourquoi on va \n",
    "# embedder X pour donner un repr√©sentation math√©matique des caract√®res via\n",
    "# des vecteurs denses\n",
    "# Avant embedding = X[i] = [5, 13, 13]\n",
    "# Apr√®s embedding = X[i] = [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# [\n",
    "      #C[5],    # embedding de 'e' ‚Üí [1.7, -0.3]\n",
    "      #C[13],   # embedding de 'm' ‚Üí [0.2, +1.5]\n",
    "      #C[13]    # embedding de 'm' ‚Üí [0.2, +1.5]\n",
    "# ]\n",
    "emb = C[X]\n",
    "# On obtient un tenseur de 32 lignes (le nombre d'exemple dans notre training set), dans chaque ligne 3 √©l√©ments (contexte) \n",
    "# et pour chaque √©l√©ment la version embedder donc une matrice de 2 dimensions\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb97c7e3-9f10-41e2-94a3-8c6fc98eff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######### CONSTRUCTION DE LA HIDDEN LAYER ############\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a78f30e-d7e3-43a0-900e-3160e368ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape emb : torch.Size([228146, 3, 2])\n",
      "shape W1 : torch.Size([6, 100])\n",
      "shape b1 : torch.Size([100])\n",
      "shape emb.view(-1, 6) : torch.Size([228146, 6])\n",
      "shape de h : torch.Size([228146, 100])\n"
     ]
    }
   ],
   "source": [
    "#Initialisatin des poids al√©atoirement\n",
    "W1 = torch.randn((6, 100)) \n",
    "# 6 lignes car \n",
    "# 6 entr√©es car notre embedding contient 6 valeurs par ligne\n",
    "# 3 lettres de contexte * 2 embedding par lettre  \n",
    "# [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# La matrice est applatit avant d'√™tre envoy√© au neuronne pour ne faire q'un seul vecteur\n",
    "# [1.7, -0.3, 0.2, 1.5, 0.2, 1.5]   # vecteur de taille 6\n",
    "\n",
    "# 100 neuronnes cach√©s avec 6 poids => Taille initi√© √† l'exp√©rience :) \n",
    "\n",
    "#Initialisation des biais al√©atoirement\n",
    "b1 = torch.randn(100)\n",
    "# 100 biais car 100 neuronnes\n",
    "#Chaque neuronne re√ßoit la valeur de l'embedding applati,\n",
    "\n",
    "\n",
    "# Il fait son calcul tanh(aX + b) et en sortie on a une valeur\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "\n",
    "#Explication du emb.view\n",
    "# A ce stade on veut muliplier une matrice emb de dimensions [32, 3, 2]\n",
    "# par la matrice W1 de dimension [6, 100]\n",
    "# Ce qui est imossible, la multiplication d'une matrice A par une matrice B impose\n",
    "# que le nombre de colonne de la matrice A soit √©gal au nombre de ligne de la matrice B\n",
    "# Il faut donc que la matrice emb ai 6 colonne (actuellement 3)\n",
    "# la fonction view permet, sans cr√©er de nouveau tenseur en m√©moire, de transoformer \n",
    "# la forme de notre matrice et pour garder une forme dynamique on indique -1 au niveau\n",
    "# de la ligne, torch sait alors que ce sera forc√©ment 32 dans notre cas car il fait\n",
    "# autmatiquement le calcul en fonction de la taille du tenseur d'origine\n",
    "# origine 32 * 3 * 2 = 192 donc si on indique 6 il fait 192 / 6 pour d√©duire 32\n",
    "\n",
    "print('shape emb :', emb.shape)\n",
    "print('shape W1 :', W1.shape)\n",
    "print('shape b1 :', b1.shape)\n",
    "print('shape emb.view(-1, 6) :', emb.view(-1, 6).shape)\n",
    "\n",
    "# La shape de h sera √©gale au nombre de ligne de la matrice emb.view(-1, 6) >> 32\n",
    "# Et du nombre de colonne de la matrice W1 >> 100\n",
    "# R√®gle de la multiplication des matrices\n",
    "\n",
    "##  R√®gle math√©matique d'addition de matrice, pour addition deux matrices il faut qu'elle soient de m√™m taille :\n",
    "# L'addition de emb.view(-1, 6) @ W1  b1 √† 1 dimension  est possible grace au\n",
    "# Broadcasting, qui va √©tendre automatiquement le plus petit vecteur pour que la forme soit compatible\n",
    "# b1 est un vecteru compos√© d'une ligne et de 100 valeurs (colonnes)\n",
    "# Pour permettre l'addition le broadcasting va r√©p√©t√© virtuellement b1 sur 32 lignes\n",
    "print('shape de h :', h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8613d0-02a1-4f23-9ba2-0caa815583a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######## CONSTRUCTION DE LA COUCHE DE SORTIE #########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21366d7-6e33-4238-ba11-b4b003eb6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisatin des poids al√©atoirement\n",
    "W2 = torch.randn((100, 27)) \n",
    "# 100 lignes car \n",
    "# 100 sorties de notre hidden layer (shape de h : torch.Size([32, 100]))\n",
    "# 27 colonnes car 27 sortie √©tant donn√©es le nombre de caract√®re possible   \n",
    "\n",
    "#Initialisation des biais al√©atoirement\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db8231-d0b2-4a22-bf49-0d21e96dfed9",
   "metadata": {},
   "source": [
    "#### 1. 27 Scores bruts non normalis√©s, un niveau \"d'energie attribu√© a chaque caract√®re\"\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "#### On va calculer la probabilit√©\n",
    "    # counts = logits.exp() # Transforme chaque score brut en nombre strictement positif \n",
    "    # on le divise par la valeur des sommes de toutes les colonnes pour en faire une moyenne ¬ª\n",
    "    # prob = counts / counts.sum(1, keepdims=True) ## Normalisation pour que la somme d'une ligne soit √©gale √† 1\n",
    "##### prob contient 32 lignes (nombre d'exemple) et 27 colonne repr√©sentant chauqe caract√®re et la probabilit√© affect√© a chaque caract√®re\n",
    "##### Cr√©ation d'un vecteur de 32 valeurs dans le quel on stock la valeur pr√©dite par le mod√®le\n",
    "##### Pour chaque caract√®re qui √©tait attendu (donc ce n'est forc√©ment celui qui a eu la meilleur probabilit√©\n",
    "##### Ce qui est normal, √† ce stade le mod√®le n'est pas encore entrain√©)\n",
    "    # preparationCalculLoss = prob[torch.arange(32), Y]\n",
    "    # print (preparationCalculLoss)\n",
    "\n",
    "##### On prend la probabilit√© du caract√®re correct, on prend son log, on met un signe moins, on moyenne :\n",
    "##### c‚Äôest la Negative Log-Likelihood, la mesure standard de ‚Äú√† quel point le mod√®le a eu tort‚Äù.\n",
    "##### C'est cette valeur que l'on va tenter de minimiser pour am√©liorer notre mod√®le\n",
    "    # loss = -preparationCalculLoss.log().mean()\n",
    "    # print (loss.item())\n",
    "\n",
    "#### La fonction F.cross_entropy(logits, y) faile calcul du neagative loglikelihood de fa√ßon plus efficiente\n",
    "#### cross_entropy fait tout en une seule op√©ration efficace au lieu de 4 quand on le fait manuellement\n",
    "#### cross_entropy est NUM√âRIQUEMENT STABLE >> \n",
    "    # Si un logit = 50 ‚Üí exp(50) ‚âà 5 √ó 10¬≤¬π ‚Üí explosion\n",
    "    # Si un logit = ‚Äì50 ‚Üí exp(‚Äì50) ‚âà 1.9e‚Äì22 ‚Üí underflow\n",
    "        # ‚û°Ô∏è aucun overflow\n",
    "        # ‚û°Ô∏è aucun underflow\n",
    "        # ‚û°Ô∏è m√™me si les logits sont √©normes\n",
    "        # ‚û°Ô∏è gradients fiables\n",
    "#### C‚Äôest plus rapide (impl√©ment√© en C++/CUDA)\n",
    "     # loss = F.cross_entropy(logits, Y)\n",
    "     # print('Caclul de la loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88020f6-1fc4-4901-8c09-747c0888ae3d",
   "metadata": {},
   "source": [
    "######################################################\n",
    "################### Entrainement #####################\n",
    "######################################################\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "# Si on ne fait pas √ßa on a cette erreur\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Afficher le nombre de param√®tre\n",
    "print('Notre mod√®le compte ',sum(p.nelement() for p in parameters), ' param√®tres')\n",
    "\n",
    "for k in range(10000): # Sur deux cents it√©rations\n",
    "\n",
    "    #Construction d'un minibach\n",
    "    # Le but ici est d'acc√©lerer l'entrainement et de consommer moins de RAM et de CPU\n",
    "    # Sans le batch, on fait entraine le r√©seau sur tout le dataset d'un coup\n",
    "    # Donc on fait plus de 220k forward et backward avec toutes les donn√©es en m√™me temps\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) #Cette ligne tire 32 indices al√©atoires dans le dataset pour construire un mini-batch.\n",
    "\n",
    "    # Remarque : \n",
    "            # ce batching implique parfois la r√©utilisatin du m√™me exemple\n",
    "            # Certains exemples ne seront pas utilis√©s (environ 40% sur des batch de 32 exemples dans 200 it√©rations)\n",
    "            # Malgr√©s cela la descente de gradiant reste bonne\n",
    "            # C'est la technique utilis√© par les LLMs comme Llama ou Gemini\n",
    "    \n",
    "    # 1) Forward pass\n",
    "    emb = C[X[ix]]    \n",
    "    #Cette ligne remplace les indices des caract√®res du mini-batch par leur embedding dense, \n",
    "    #cr√©ant un tenseur [32,3,2] pr√™t pour la couche cach√©e.\n",
    "    \n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)   # Hidden layer [32, 100]\n",
    "    logits = h @ W2 + b2           # Couche de sortie [32, 27] \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # Calcul de la loss (exp + counts / counts.sum ...) \n",
    "\n",
    "    # 2) Backprop\n",
    "    # Les gradients sont g√©n√©r√©s et stock√©s automatiquement par loss.backward() \n",
    "    # et on les efface manuellement juste apr√®s les avoir utilis√©s pour mettre √† jour les poids\n",
    "    for p in parameters:\n",
    "        p.grad = None  \n",
    "    \n",
    "    loss.backward() \n",
    "\n",
    "    # 3) Gradient descent >> Mise √† jour des poids\n",
    "    for p in parameters:\n",
    "        p.data += -0.001 * p.grad    # descente de gradiant\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e4bf0-bef6-495c-9b37-ef1dd9737020",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">üî• Note sur la d√©finition du learning rate</h2>\n",
    "\n",
    "Pour trouver une bonne valeur du learning rate, La m√©thode utilis√©e en deep learning moderne :\n",
    "\n",
    "On teste plusieurs learning rates :\n",
    "par ex. [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "On lance quelques it√©rations pour chaque valeur\n",
    "\n",
    "On observe la courbe de loss\n",
    "\n",
    "‚úî Le bon learning rate est : \n",
    "celui o√π la loss baisse rapidement sans instabilit√© et sans oscillations violentes\n",
    "\n",
    "### üî• Qu‚Äôest-ce que torch.linspace ?\n",
    "\n",
    "#torch.linspace(start, end, steps) cr√©e un vecteur de valeurs r√©guli√®rement espac√©es entre start et end.\n",
    "torch.linspace(0, 1, 5)\n",
    "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]) >> 5 valeurs √©galement esp√©c√©es entre 0 et 1\n",
    "\n",
    "Dans notre exp√©rimentation, on va tester plusieurs valeurs possibles du learning rate.\n",
    " Le principe :\n",
    " - On boucle sur ~100 it√©rations d'entra√Ænement pour chaque learning rate test√©\n",
    " - √Ä chaque essai, on enregistre : \n",
    "       ‚Ä¢ la valeur du learning rate utilis√©\n",
    "       ‚Ä¢ la loss obtenue apr√®s ces it√©rations\n",
    " - Ensuite, on trace une courbe (loss en fonction du learning rate)\n",
    "   afin d‚Äôidentifier la valeur qui permet au mod√®le d‚Äôapprendre le plus efficacement.\n",
    "\n",
    " Cette m√©thode est une pratique courante en deep learning : elle permet de choisir\n",
    " un learning rate qui fait descendre la loss rapidement, sans instabilit√©s.\n",
    "\n",
    "Une fois le learning rate optimal identifi√©, on entra√Æne le mod√®le avec cette valeur jusqu‚Äô√† atteindre un palier (plateau de loss).\n",
    "Pour affiner encore la convergence, on r√©duit ensuite le learning rate d‚Äôun facteur 10 et on continue l‚Äôentra√Ænement : cela permet souvent de diminuer la loss un peu plus finement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adcc719-b2d5-4594-aada-c9ce11c697da",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">üî• Le standart dans les entrainements de mod√®le est de d√©couper le dataset en 3 parties\n",
    "</h2>\n",
    "\n",
    "#### ---> La partie entrainement (80% des donn√©es)\n",
    " ####     -  pour entrainer les param√®tres du mod√®le C, W1, W2, b1, b2\n",
    "#### ---> La partie dev / validation (10% des donn√©es)\n",
    " ####     -pour tester les hyperparam√®tres learning rate, taille du batch, dimensions de embeddings, taille du hiden                layer, architecture\n",
    "#### ---> La partie test (10% des donn√©es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f2b1b5-7732-418d-9021-9f93d18aa8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626\n",
      "28829\n",
      "dataset d'entrainement :  182625  trigrammes\n",
      "dataset de dev / Validation :  22655  trigrammes\n",
      "dataset de test :  22866  trigrammes\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "## Cr√©ation des datasets d'entainement/ dev / test ###\n",
    "######################################################\n",
    "\n",
    "# Cr√©ation d'une fonction r√©utilisatble\n",
    "def build_dataset(words):\n",
    "    block_size = 3 # taille du contexte dans cet exemple \n",
    "    \n",
    "    X, Y = [], [] #Cr√©ation de deux liste X pour les entr√©es et Y pour la sortie attendue\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "        for ch in w + '.': # Pour chauqe catact√®re du mot\n",
    "    \n",
    "            ix = stoi[ch] # On r√©cup√®re la valeur du nombre correspondant √† la lettre\n",
    "            X.append(context) # On ajoute le contexte √† la liste X\n",
    "            Y.append(ix) # On ajoute la valeur num√©rique du caract√®re √† la liste Y\n",
    "            \n",
    "            # On jette le premier √©l√©ment du contexte et on d&calle tout a gauche\n",
    "            #et on ajoute la valeur num√©rique de la lettre dans le contexte\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    #print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# Utilisation de la fonction pour cr√©er les diff√©rents dataset\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words) # On m√©lange al√©atoirment la liste de mot\n",
    "n1 = int(0.8*len(words))\n",
    "print(n1)\n",
    "n2 = int(0.9*len(words))\n",
    "print(n2)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # On cr√©er une liste avec les mots compris dans le mot 1 et le mot 25626 (80%)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # On cr√©er une liste avec les mots compris  entre le mot 25626 et le mot 28829 (10%)\n",
    "Xte, Yte = build_dataset(words[n2:]) # On cr√©er une liste avc les mots apr√®s 28829 (10%)\n",
    "\n",
    "print('dataset d\\'entrainement : ', Xtr.shape[0], ' trigrammes')\n",
    "print('dataset de dev / Validation : ', Xdev.shape[0], ' trigrammes')\n",
    "print('dataset de test : ', Xte.shape[0], ' trigrammes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d924cb54-d011-4be0-828c-4f3dcdcf7aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre mod√®le compte  18167  param√®tres\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "################### PARAM√àTRES #######################\n",
    "######################################################\n",
    "pw1 = 200 ## Nb de poids dans la hidden couche\n",
    "pemb = 20 ## Nb de dimensions dans l'embeddding\n",
    "nbcar = 27 ## Nb de caract√®res\n",
    "pembXtrig = pemb * block_size # la taille du contexte * la taille de l'embedding\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((nbcar, pemb)) # Couche d'embedding\n",
    "W1 = torch.randn((pembXtrig, pw1)) # Poids Hidden Layer\n",
    "b1 = torch.randn(pw1) # Biais Hidden layer\n",
    "W2 = torch.randn((pw1, nbcar)) # Poids sortie\n",
    "b2 = torch.randn(nbcar) # Biais sortie\n",
    "parameters = [C, W1, W2, b1, b2]\n",
    "\n",
    "# Afficher le nombre de param√®tre\n",
    "print('Notre mod√®le compte ',sum(p.nelement() for p in parameters), ' param√®tres')\n",
    "\n",
    "# Si on ne fait pas √ßa on a cette erreur\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "lre = torch.linspace (-3, 0, 60000) ##g√©n√®re 60000 valeurs iniform√©ment r√©parties entre -3 et 0\n",
    "#print (lre) # tensor([-3.0000, -2.9970, -2.9940, -2.9910, -2.9880, -2.9850, ...]\n",
    "lrs = 10**lre # Exponentiel en base 10 √† chaque √©l√©ment > Transforme les exposants lre en learning rates\n",
    "# Permte de tester sur diff√©rents ordre de grandeur (0,1 / 0,01 / 0,001 ...)  pliutot que sue un espacement lin√©aire (-3.0000, -2.9970, -2.9940)\n",
    "#print (lrs) # ([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac2cfef-aca8-4347-877e-a7abe7aeb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri, lossi, stepi = [], [], [] ## Stockage des learning rates essay√© avec les les loss asosci√©s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b8d985-0eb3-481d-b654-3947a101c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "################### ENTRAINEMENT #####################\n",
    "######################################################\n",
    "batchSize = 128\n",
    "\n",
    "for i in range(150000): \n",
    "\n",
    "    #Construction d'un minibach\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batchSize,)) #Cette ligne tire 32 indices al√©atoires dans le dataset pour construire un mini-batch.\n",
    "    \n",
    "    # 1) Forward pass\n",
    "\n",
    "    # 1.1) Embedding\n",
    "    emb = C[Xtr[ix]]    \n",
    "    #Cette ligne remplace les indices des caract√®res du mini-batch par leur embedding dense, \n",
    "    #cr√©ant un tenseur [32,3,2] pr√™t pour la couche cach√©e.\n",
    "\n",
    "    # 1.2) Hidden layer\n",
    "    h = torch.tanh(emb.view(-1, pembXtrig) @ W1 + b1)   # Hidden layer [32, 100]\n",
    "\n",
    "    # 1.3) Sortie \n",
    "    logits = h @ W2 + b2           # Couche de sortie [32, 27] \n",
    "    \n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # Calcul de la loss (exp + counts / counts.sum ...) \n",
    "\n",
    "    # 2) Backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None  \n",
    "    loss.backward() \n",
    "\n",
    "    # 3) Mise √† jour des lri et lossi\n",
    "    #lr = lrs[i]\n",
    "    if i <= 120000:\n",
    "        lr = 0.15\n",
    "    \n",
    "    if i >120000:\n",
    "        lr = 0.015\n",
    "    \n",
    "    # 3) Gradient descent >> Mise √† jour des poids\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad    # descente de gradiant\n",
    "\n",
    "    # Track stats\n",
    "    #lri.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "## Affich√© la courbe avec matplot des learning rates et loss pour voir ou se situe le learnig rate le plus efficace et stable\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.plot(lrs[:len(lossi)], lossi)\n",
    "#plt.xscale('log')\n",
    "#plt.xlabel('Learning rate (log scale)')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.title('LR Finder ‚Äì Evolution de la loss en fonction du learning rate')\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "\n",
    "#print('Loss Entrainement: ',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb9f4cd8-192a-4f58-80ae-4ffb0eb5dbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa93c5b2860>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "441729aa-01c5-4f05-8af0-ca040994fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Fin entrainement :  2.0389721393585205\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "###### EVALUATION AVEC DONN√âES D'ENTRAINEMENT ########\n",
    "######################################################\n",
    "# Embedding\n",
    "emb = C[Xtr]\n",
    "# 1.2) Hidden layer [32, 100]\n",
    "h = torch.tanh(emb.view(-1, pembXtrig) @ W1 + b1)  \n",
    "# 1.3) Sortie [32, 27]\n",
    "logits = h @ W2 + b2      \n",
    "lossEval = F.cross_entropy(logits, Ytr)\n",
    "print('Loss Fin entrainement : ',lossEval.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce934d6-3c2a-486e-80a6-c1da55a93496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22655, 3, 20])\n",
      "Loss Dev:  2.1271073818206787\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "###### EVALUATION AVEC DONN√âES DE VALIDATION #########\n",
    "######################################################\n",
    "# Embedding\n",
    "emb = C[Xdev]\n",
    "print(emb.shape)\n",
    "# 1.2) Hidden layer [32, 100]\n",
    "h = torch.tanh(emb.view(-1, pembXtrig) @ W1 + b1)  \n",
    "# 1.3) Sortie [32, 27]\n",
    "logits = h @ W2 + b2      \n",
    "lossEval = F.cross_entropy(logits, Ydev)\n",
    "print('Loss Dev: ',lossEval.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833dc09d-4e2c-44c3-afd6-0fb7fd4c4dc9",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">üî• Note sur l'entrainement</h2>\n",
    "\n",
    "### Il faut tester diff√©rentes variations de hyperparam√®tres \n",
    "#### * Variation de la taille des embeddiings des caract√®res\n",
    "#### * Variation de la taille du hiden layer (nombre de neuronne)\n",
    "#### * Refaire le calcul du learning rate le plus adapt√© lors des chnagements\n",
    "\n",
    "### L'objectif √©tant d'avoir la losss la plus basse possible\n",
    "\n",
    "<table border=\"1\" cellpadding=\"8\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <thead style=\"background-color:#blue;\">\n",
    "    <tr>\n",
    "      <th>Mesure</th>\n",
    "      <th>Embedding 2 dims<br>Hidden 100 neurones</th>\n",
    "      <th>Embedding 10 dims<br>Hidden 300 neurones</th>\n",
    "      <th>Embedding 20 dims<br>Hidden 100 neurones</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>Total param√®tres</b></td>\n",
    "      <td> 3481 </td>\n",
    "      <td> 17697 </td>\n",
    "      <td> 18167 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Learning rate utilis√©</b></td>\n",
    "      <td> 0.1 </td>\n",
    "      <td> 0.15 </td>\n",
    "      <td> 0.15 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Taille du batch utilis√©</b></td>\n",
    "      <td> 32 </td>\n",
    "      <td> 32 </td>\n",
    "      <td> 128 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Traitement compl√©mentaire avec affinage du learning rate</b></td>\n",
    "      <td> 0.01 </td>\n",
    "      <td> 0.015 </td>\n",
    "      <td> 0.015 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Loss obtenue</b></td>\n",
    "      <td> 2.6</td>\n",
    "      <td> 2.06</td>\n",
    "      <td> 2.03</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8de7423d-6a10-4a8e-bdc7-9e9a491b2b2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahela\n",
      "jhovi\n",
      "kimri\n",
      "revolahnanden\n",
      "jazonte\n",
      "delynn\n",
      "jareen\n",
      "nellara\n",
      "chaily\n",
      "kaleigh\n",
      "ham\n",
      "pory\n",
      "quinn\n",
      "sulie\n",
      "alianni\n",
      "wavero\n",
      "dearyn\n",
      "kai\n",
      "eveigh\n",
      "brex\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "########### TEST DE G√âN√âRATION DE DONN√âES ##############\n",
    "########################################################\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):   # g√©n√©rer 5 mots\n",
    "    out = []\n",
    "    context = [0] * block_size   # [0, 0, 0] au d√©but\n",
    "\n",
    "    while True:\n",
    "        # 1) Pr√©parer l'entr√©e : m√™me format que Xtr ( [1, 3] )\n",
    "        x = torch.tensor([context])      # shape [1, 3]\n",
    "        emb = C[x]                       # shape [1, 3, pemb] = [1, 3, 20]\n",
    "\n",
    "        # 2) Forward dans le MLP\n",
    "        h = torch.tanh(emb.view(-1, pembXtrig) @ W1 + b1)   # [1, pw1]\n",
    "        logits = h @ W2 + b2                                # [1, nbcar]\n",
    "\n",
    "        # 3) Softmax + tirage\n",
    "        probs = F.softmax(logits, dim=1)                    # [1, nbcar]\n",
    "        ix = torch.multinomial(probs[0], num_samples=1, generator=g).item()\n",
    "\n",
    "        # 4) Mise √† jour du contexte : on glisse la fen√™tre\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "        # 5) Fin de mot ?\n",
    "        if ix == 0:   # '.' -> token de fin\n",
    "            break\n",
    "\n",
    "        out.append(itos[ix])\n",
    "\n",
    "    print(\"\".join(out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
