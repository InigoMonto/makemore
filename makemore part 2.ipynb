{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df29115-a89e-48d5-97f7-3065f9bbfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6ce3f-2178-4114-acc0-cc20b25554e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lecture de tous les mots\n",
    "words = open('dataset/names.txt', 'r').read().splitlines()\n",
    "words [:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a36898-1c35-4e83-a45d-3a9df2878544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#Création des dictionnaires\n",
    "chars = sorted(list(set(''.join(words)))) # estraction de chaque caracère de la liste de mots\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # Création d'un liste en commençant à l'indice 1 ou chaque caractère est associcé à un index\n",
    "stoi['.'] = 0 #jout du caractère \".\" à l'indice 0\n",
    "itos = {i:s for s,i in stoi.items()} # Création d'une liste inverse pour avoir le nombre et la corespondance du caractere\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c390758-3d4d-40f4-a874-530d13b928f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "#Construction du dataset\n",
    "block_size = 3\n",
    "\n",
    "X, Y = [], [] #Création de deux liste X pour les entrées et Y pour la sortie attendue\n",
    "\n",
    "for w in words [:5]:\n",
    "    print (w)\n",
    "    context = [0] * block_size # On initie le contexte avec [0, 0, 0]\n",
    "    for ch in w + '.': # Pour chauqe catactère du mot\n",
    "\n",
    "        ix = stoi[ch] # On récupère la valeur du nombre correspondant à la lettre\n",
    "        X.append(context) # On ajoute le contexte à la liste X\n",
    "        Y.append(ix) # On ajoute la valeur numérique du caractère à la liste Y\n",
    "        \n",
    "        #On affiche le contexte + ----> + le caractère du mot que l'on traite \n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        # La même chose mais on affiche les indexes\n",
    "        #print(''.join(str(i) for i in context), '--->', ix)\n",
    "\n",
    "        \n",
    "        \n",
    "        # On jette le premier élément du contexte on d&calle tout a gauche\n",
    "        #et on ajoute la valeur numérique de la lettre dans le contexte\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec19080-9bac-4cc5-b7fe-cb6a28393e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a73b41e-d517-43af-878f-dd0c7e0ff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de la matrice des embeddings, ou chaque caractère est \n",
    "# représenté par une matrice a deux dimensions\n",
    "C = torch.randn((27, 2))\n",
    "#print (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16eae17-a2a4-4719-b13a-a5cdacf0301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "####### CONSTRUCTION DE LA COUCHE D'EMBEDDING ########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2ba1ae9-523d-4ff3-b9d9-a739a36c1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding de la matric X\n",
    "# A ce stade X contient les contexte de 3 index\n",
    "# Le réseau ne peut pas travailler avec ces indexes c'est pourquoi on va \n",
    "# embedder X pour donner un représentation mathématique des caractères via\n",
    "# des vecteurs denses\n",
    "# Avant embedding = X[i] = [5, 13, 13]\n",
    "# Après embedding [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# [\n",
    "      #C[5],    # embedding de 'e' → [1.7, -0.3]\n",
    "      #C[13],   # embedding de 'm' → [0.2, +1.5]\n",
    "      #C[13]    # embedding de 'm' → [0.2, +1.5]\n",
    "# ]\n",
    "emb = C[X]\n",
    "# On obtient un tenseur de 32 lignes, dans chaque ligne 3 éléments (contexte) \n",
    "# et pour chaque élément la version embedder donc une matrice de 2 dimensions\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb97c7e3-9f10-41e2-94a3-8c6fc98eff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######### CONSTRUCTION DE LA HIDDEN LAYER ############\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a78f30e-d7e3-43a0-900e-3160e368ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape emb : torch.Size([32, 3, 2])\n",
      "shape W1 : torch.Size([6, 100])\n",
      "shape emb.view(-1, 6) : torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "#Initialisatin des poids aléatoirement\n",
    "W1 = torch.randn((6, 100)) \n",
    "# 6 lignes car \n",
    "# 6 entrées car notre embedding contient 6 valeurs par ligne \n",
    "# 3 lettres de contexte * 2 embedding par lettre  \n",
    "# [[1.7, -0.3], [0.2, +1.5], [0.2, +1.5]]\n",
    "# La matrice est applatit avant d'être envoyé au neuronne pour ne faire q'un seul vecteur\n",
    "# [1.7, -0.3, 0.2, 1.5, 0.2, 1.5]   # vecteur de taille 6\n",
    "\n",
    "# 100 neuronnes cachés avec 6 poids\n",
    "\n",
    "#Initialisation des biais aléatoirement\n",
    "b1 = torch.randn(100)\n",
    "# 100 biais car 100 neuronnes\n",
    "#Chaque neuronne reçoit la valeur de l'embedding applati,\n",
    "\n",
    "\n",
    "# Il fait son calcul tanh(aX + b) et en sortie on a une valeur\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "\n",
    "#Explication du emb.view\n",
    "# A ce stade on veut muliplier une matrice emb de dimensions [32, 3, 2]\n",
    "# par la matrice W1 de dimension [6, 100]\n",
    "# Ce qui est imossible, la multiplication d'une matrice A par une matrice B impose\n",
    "# que le nombre de colonne de la matrice A soit égal au nombre de ligne de la matrice B\n",
    "# Il faut donc que la matrice emb ai 6 colonne\n",
    "# la fonction view permet, sans créer de nouveau tenseur en mémoire de transoformer \n",
    "# la forme de notre matrice et pour garder un forme dynamique on indique -1 au niveau\n",
    "# de la ligne, torch sait alors que ce sera forcéement 32 dans notre cas car il fait\n",
    "# autmatiquement le calcul en fonction de la taille du tenseur d'origine\n",
    "# origine 32 * 3 * 2 = 192 donc si on indique 6 il fait 192 / 6 pour déduire 32\n",
    "\n",
    "print('shape emb :', emb.shape)\n",
    "print('shape W1 :', W1.shape)\n",
    "print('shape emb.view(-1, 6) :', emb.view(-1, 6).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a8613d0-02a1-4f23-9ba2-0caa815583a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "######## CONSTRUCTION DE LA COUCHE DE SORTIE #########\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21366d7-6e33-4238-ba11-b4b003eb6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reprendre la vidéo à 29 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
